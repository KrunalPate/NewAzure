# Module 9: Initializing and Optimizing Machine Learning Models
# Lab: Initializing and optimizing Machine Learning models

### Scenario
You work as a data scientist for Adatum Consultants, a company that provides machine learning services and advice for a range of clients. One of your clients runs an e-commerce company and they want to show a different home page to users depending on their annual income. Your client has, therefore, asked you to test and evaluate a machine learning model that predicts adult incomes.
As part of this evaluation, you need to compare a tuned model with an untuned model, and investigate the effects of using hyperparameters for tuning.

### Objectives
After completing this lab, you will be able to:
-   Use hyperparameters, and evaluate the effects of tuning on model predictions.

### Lab Setup
Estimated Time: 45 minutes
Virtual machine: **20774A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

## Exercise 1: Using hyperparameters

### Scenario
In this exercise, you will use the **Tune Model Hyperparameters** module to select automatically the parameters that lead to the best accuracy for a classification model. The experiment will compare two support vector machine (SVM) classifiers:
-   One is optimized by the **Tune Model Hyperparameters** module.
-   One is trained by using the default parameters for the **Two-Class Support Vector Model** module, and then trained by using the **Train Model** module.

The dataset that is used in this experiment, which is included in the Samples Gallery of Machine Learning Studio, is **Adult Census Income Binary Classification**. The model will attempt to predict the **income** value.

The main tasks for this exercise are as follows:
1. Create a model with tuned parameters
2. Add an untuned model
3. Compare tuned and untuned models

#### Task 1: Create a model with tuned parameters
1.  In Machine Learning Studio, create a new experiment, and then add the **Samples - Adult Census Income Binary Classification dataset**.
2.  Add and connect a **Split Data** module. Split 80 percent of the data into a training dataset.
3.  Add a **Two-Class Support Vector Machine** module. Configure it to use the following iterations: **5,10,15**.
4.  Add and connect a second **Split Data** module. Split 70 percent of the data into a training dataset. The remaining 30 percent will be used for validation.
5.  Add and connect a **Tune Model Hyperparameters** module to the outputs of the modules added in steps 3 and 4. Configure the module to sweep the entire grid for values of the **income** column.
6.  Add and connect a **Score Model** module to the second output of the module created in step 5 and the second output of the module created in step 2.
7.  Add and connect an **Evaluate Model** module to the output of the module added in step 6.
8.  Save the experiment as **Hyperparameters Test**.

#### Task 2: Add an untuned model
1.  Add a **Two-Class Support Vector Machine** module with default settings.
2.  Add a **Train Model** module, connected to the **Two-Class Support Vector Machine** module created in the previous step and to the training dataset as inputs.
3.  Train the model on the **income** column.
4.  Add a **Score Model** module, and then connect the output of the **Train Model** module and the testing dataset as inputs.
5.  Connect the output of the module to the **Evaluate Model** module already in the experiment.

#### Task 3: Compare tuned and untuned models
1.  Run the experiment and compare the results.
2.  Does tuning the hyperparameters improve the accuracy of the model in this case?
The answer is that tuning hyperparameters has minimal effect in this case. The **Scored dataset** line represents the left input of the **Evaluate Model** module, and uses tuned parameters. The tuned parameters have not produced better accuracy than the untuned model (the right input, the **Scored dataset to compare** line); the ROC curves are almost the same for each input.

**Results**: At the end of this exercise, you will have created an experiment that compares the effect of tuned hyperparameters with a simple classification model that does not use tuning.

**Question:** What are the input and output nodes of the **Tune Model Hyperparameters** module?

**Question:** Why did you use two **Split Data** modules in the lab experiment?

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
