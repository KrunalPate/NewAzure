# Module 8: Using R and Python with Azure Machine Learning
# Lab: Using R and Python with Machine Learning

### Scenario
You work as a data scientist for Adatum Consultants, a company that provides machine learning services and advice for a range of clients. One of your clients is Wide World Importers, who are transitioning from a Business Intelligence practice to a Business Analytics practice. As part of this transition, Wide World Importers are looking for insights from operational data sources throughout the organization.
You are working with a team of Wide World Importers business analysts, who write reports using data that is stored in a variety of locations, including Azure SQL Database. The business analysts want to investigate the potential for developing custom models that make use of R and Python scripts, and how to document processes by using a Jupyter notebook.

### Objectives
After completing this lab, you will be able to:
-   Prepare an Azure SQL Database with imported data.
-   Explore data using R and document the process using a Jupyter notebook.
-   Analyze data using Python as part of a Machine Learning experiment.

### Lab Setup
Estimated Time: 90 minutes
Virtual machine: **20774A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**
Ensure that the **MT17B-WS2016-NAT**, **20774A-LON-DC**, and **20774A-LON-DEV** virtual machines are running, and that you are logged on to **20774A-LON-DEV**.

## Exercise 1: Preparing the lab

### Scenario
In this exercise, you will create a database, and import sample data into this database. You will use this database in subsequent exercises in this lab.

The main tasks for this exercise are as follows:
1. Create Azure SQL Database
2. Import data into Azure SQL Database

#### Task 1: Create Azure SQL Database
1.  Log on to **20774A-LON-DEV** virtual machine as **ADATUM\\AdatumAdmin**.
2.  In Internet Explorer, browse to the Azure Portal and sign in with your Azure account.
3.  Create a new SQL Database called WideWorldImporters-Standard, using a new resource group called &lt;yourname&gt;rg, using a blank database source.
4.  Call the server name **&lt;yourname&gt;&lt;date&gt;**, create a login called dbadmin with a password of **Pa55w.rd** in your local region.
5.  Select a standard pricing tier with a DTU of 10(S0).
6.  Note the database and server names you created.
7.  Wait for the successful deployment message.
8.  On the database blade, change the Firewall settings to add your current client IP address to the list of allowed IP addresses.

#### Task 2: Import data into Azure SQL Database
1.  On the **20774A-LON-DEV** virtual machine, start a command prompt with administrative rights.
2.  Change to the **C:\\Program Files (x86)\\Microsoft SQL Server\\130\\DAC\\Bin** folder.
3.  Run the command in **E:\\Labfiles\\Lab08\\SqlPackageCmd.txt**.
4.  When you have imported the database successfully, close the command prompt.

**Results**: At the end of this exercise, you will have created an Azure SQL Database, and imported WideWorldImporters-Standard data into this database.

## Exercise 2: Exploring data using R

### Scenario
The business analysts at Wide World Importers are interested in exploring the orders data in the database to determine which products that they export are the most popular in terms of units sold, and total sales value. They also want to ascertain whether there is any seasonal variation in the sales of these products.

The main tasks for this exercise are as follows:
1. Prepare the product sales data
2. Determine the most popular products in the dataset
3. Investigate how product sales vary over time
4. Examine the data to see whether there is a pattern to product sales
5. Run the Jupyter notebook

#### Task 1: Prepare the product sales data
1.  The data you require for this exercise is in the **Sales.OrderLines** table in the Wide World Importers database. You need to export this data as a CSV file so that you can load it into a Jupyter notebook. To do this, create a new experiment using Microsoft Azure Machine Learning Studio.
2.  In the new experiment, add an Import Data module that connects to your Wide World Importers database and uses the following SQL SELECT statement to retrieve the data:
    ```
	SELECT StockItemID, Description, Quantity, UnitPrice = CAST(UnitPrice AS INTEGER), PickingCompletedWhen
	FROM Sales.OrderLines
    ```
3.  Add a **Convert to CSV** module to the experiment, and connect its input port to the output port of the **Import Data** module.
4.  Run the experiment to perform the data import and conversion.
5.  Save the imported and converted data as a new dataset named **Wide World Importers**.

#### Task 2: Determine the most popular products in the dataset
>**Note:** You can find the comments and code for this lab in the file **Product Sales.txt**, located in the **E:\\Labfiles\\Lab08** folder.

01. In Microsoft Azure Machine Learning Studio, move to the datasets list, and open the Wide World Importers dataset using an R notebook.
02. Examine the notebook. It contains two cells containing R code. The code in the first cell downloads the data from the Wide World Importers CSV file into the notebook, and the second cell displays the top six rows from the dataset.
03. Add a new Markdown cell above the first code cell and add the following comment that describes the purpose of the notebook. If you run this cell after entering the text, you will see how the Markdown formatting is applied:
    ```
	# Evaluate Product Sales
	Exploratory exercises over the product orders data for Wide World Importers
	- What are the top 20 most popular products?
	- How do sales of the most popular product vary over time?
	- Is there a pattern to these sales?
	## What are the most popular products?
	Load the libraries required to perform these tasks
    ```
04. Add another cell below this top cell (and above the existing cells). Add the following code to this cell. This code loads the libraries that will be used by the R code in the notebook:
    ```
	library(AzureML)
	library(dplyr)
	library(ggplot2)
	library(scales)
	install.packages("xts") # You also need to install the xts package
	library(xts)
	install.packages("broom") # And the broom package
	library(broom)
    ```
05. Run the code in this cell. A number of messages should appear indicating that the libraries are being loaded.
06. Click in the cell containing the following code. This code loads the data:
    ```
	library("AzureML")
	ws <- workspace()
	dat <- download.datasets(ws, "Wide World Importers")
    ```
07. Change the last line of this code, and add an additional line at the end, as highlighted here:
    ```
	library("AzureML")
	ws <- workspace()
	**ordersDataSet** <- download.datasets(ws, "Wide World Importers")
	**head(ordersDataSet)**
    ```
08. Run the code in this cell. It should load the data and display the first six rows.
09. Locate the cell containing the following code:
    ```
	head(dat)
    ```
10. This code is no longer necessary. Change it to a Markdown cell with the following text:
    ```
	The same product might occur in more than one order. Generate a list of products and their descriptions that have been ordered at least once. Each product should occur only once in the list. Store the result in another dataset called *stockItemsDataSet*.
    ```
11. Insert a new code cell below this text and add the following code. This code generates a list of stock item IDs and descriptions, removing any duplicates:
    ```
	ordersDataSet %>%
	select(StockItemID, Description) %>%
	distinct() -> stockItemsDataSet
	head(stockItemsDataSet)
    ```
12. Run the code in this cell and verify that the **StockItemID** and **Description** columns for the first six items appear.
13. A new code cell will have been created automatically at the end of the notebook. Change this cell to a Markdown cell and add the following text:
    ```
	Find which products are the most popular, using a *dplyr* pipeline as follows:
	- Extract the StockItemID and quantity sold for each order
	- Group the results by StockItemID
	- Sum the quantity sold for each group, and name the column as *numOrdered*
	- Order the results in descending order of *numOrdered* (the most popular items will be at the top of the list)
	- Limit the results to the first 20 rows
	Save the results in another dataset named *numOrderedDataSet*
    ```
14. Insert another code cell at the end of the notebook, and add the following code. This groups the data by the stock item ID, and then calculates the total sales for each group. The final result is filtered to only contain the 20 most popular products:
    ```
	ordersDataSet %>%
	select(StockItemID, Quantity) %>%
	group_by(StockItemID) %>%
	summarize(numOrdered = sum(Quantity)) %>%
	arrange(desc(numOrdered)) %>%
	filter(row_number() <= 20) -> numOrderedDataSet
	head(numOrderedDataSet)
    ```
15. Run the code in this cell. Verify that the **StockItemID** and **numOrdered** columns for the first six items appear.
16. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	Combine the *numOrderedDataSet* and the *stockItemsDataSet* over the *StockItemID* column so that the list contains the *StockItemID*, *numOrdered*, and *Description* columns
    ```
17. Insert another code cell at the end of the notebook, and add the following code. This code adds the item description back into the dataset:
    ```
	results <- inner_join(numOrderedDataSet, stockItemsDataSet, by = "StockItemID")
	head(results)
    ```
18. Run the code in this cell. Verify that the **StockItemID**, **numOrdered**, and **Description** columns for the first six items appear.
19. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	Generate a graph showing the product descriptions on the X-axis and the number ordered on the Y-axis
    ```
20. Insert another code cell at the end of the notebook, and add the following code. This code plots a graph of product sales:
    ```
	options(repr.plot.width=10, repr.plot.height=8)
	ggplot(results) +
	geom_point(mapping = aes(x = reorder(Description, -numOrdered), y = numOrdered), color = "red", size = 5) +
	labs(x = "Product", y = "Total Ordered") +
	scale_x_discrete(labels = function(x) { lapply(strwrap(x, width = 25, simplify = FALSE), paste, collapse = "\n")}) +
	theme(axis.text.x = element_text(angle = 90, size = 10))
    ```
21. Run the code in this cell. Verify that a graph appears showing the sales for the top 20 most popular products, in order of total sales.

#### Task 3: Investigate how product sales vary over time
01. The graph in the previous exercise showed that product 191 had the most orders. You now want to see how sales of this item vary over time. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	## How do product sales vary over time?
	Product 191 (Black and orange fragile despatch tape 48mmx75m) is the most popular product. How do sales of this product vary over time (if at all)?
	Find all sales of product 191.
    ```
02. Insert another code cell at the end of the notebook, and add the following code. This code filters the dataset so that it only contains the data for product 191:
    ```
	ordersDataSet %>%
	filter(StockItemID == 191) -> productSalesDataSet
	head(productSalesDataSet)
	str(productSalesDataSet)
    ```
03. Run the code in this cell and verify that the results only contain sales for product 191.
04. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	Reorganize the data as a time series, based on the *PickingCompletedWhen* column. Note that this column is actually a string (chr) in month/day/year format, so it needs to be converted to a datetime.
    ```
05. Insert another code cell at the end of the notebook, and add the following code. This code reorganizes the data as a time series indexed by the **PickingCompletedWhen** column:
    ```
	ordersDataSet %>%
	timeSeriesOrders <- xts(productSalesDataSet[c("StockItemID", "Quantity")],
	order.by = as.POSIXct(productSalesDataSet$PickingCompletedWhen, format = "%m/%d/%Y"))
	head(timeSeriesOrders)
    ```
06. Run the code in this cell and verify that the data is now indexed in date order.
07. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	Work out how many units were ordered each day.
    ```
08. Insert another code cell at the end of the notebook, and add the following code. This code uses the **apply.daily** function of the time series to calculate the sum of product sales by day:
    ```
	dailyTotals <- apply.daily(timeSeriesOrders, function(x){ apply(x$Quantity, 2, sum)})
	head(dailyTotals)
    ```
09. Run the code in this cell. The results should show a list of total sales for each day.
10. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	Plot a graph showing how the sales vary by day.
	First, the time series must be converted back into a dataframe to work with ggplot.
    ```
11. Insert another code cell at the end of the notebook, and add the following code. This code uses the **tidy** function to convert the time series back into a dataframe:
    ```
	graphData <- tidy(dailyTotals)
	head(graphData)
    ```
12. Run the code in this cell. Six rows of data should appear, although they will not be particularly meaningful because the first two columns display the data using an internal format.
13. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	Then convert the index column back into a datetime value.
    ```
14. Insert another code cell at the end of the notebook, and add the following code. This code converts the **index** column in the dataframe back into a datetime value:
    ```
	graphData$index <- as.POSIXct(graphData$index, origin="1970-01-01")
	head(graphData)
    ```
15. Run the code in this cell. The index column should display recognizable dates.
16. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	Finally, plot the data.
    ```
17. Insert another code cell at the end of the notebook, and add the following code. This code plots a graph of sales for product 191 in date order:
    ```
	options(repr.plot.width=15, repr.plot.height=5)
	ggplot(graphData, aes(x = index, y = value)) +
	ggtitle("Product Sales Over Time for Product 191") +
	xlab("Date") +
	ylab("Units Sold") +
	scale_x_datetime(labels = date_format("%Y-%m"), breaks = date_breaks("months")) +
	theme(axis.text.x = element_text(angle = 45)) +
	geom_point(color = "blue")
    ```
18. Run the code in this cell. The graph should appear. Note that it shows data for several years.

#### Task 4: Examine the data to see whether there is a pattern to product sales
01. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	## Is there a pattern to these sales?
	Focus on sales for 2013.
    ```
02. Insert another code cell at the end of the notebook, and add the following code. This code filters the data and creates a new dataset containing only the sales for product 191 for 2013. The code then uses this data to plot the same graph as before:
    ```
	firstDay <- as.POSIXct("2013-01-01")
	lastDay <- as.POSIXct("2013-12-31")
	day1 <- as.numeric(firstDay)
	dayN <- as.numeric(lastDay)
	productSalesFor2013 <- filter(graphData, (index >= day1) & (index <= dayN))
	options(repr.plot.width=15, repr.plot.height=5)
	ggplot(productSalesFor2013, aes(x = index, y = value)) +
	ggtitle("Product Sales for Product 191 in 2013") +
	xlab("Date") +
	ylab("Units Sold") +
	scale_x_datetime(labels = date_format("%Y-%m"), breaks = date_breaks("months")) +
	theme(axis.text.x = element_text(angle = 45)) +
	geom_point(color = "blue")
    ```
03. Run the code in this cell.
04. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	Fit a curve to the sales, and then compare to curves for 2014 and 2015 to see if there is a regular pattern.
    ```
05. Insert another code cell at the end of the notebook, and add the following code. This code plots the graph again, but adds an overlay showing a curve fitted to the data. The curve is generated by using the **lm** function to perform polynomial regression over the sales figures. The curve generated is based on the formula y ~ x^9 (y is some function of the 9<sup>th</sup> power of x), and the **lm** function works out the appropriate coefficients that most closely match the data. The 9<sup>th</sup> power of x was selected after trial and error with other powers. Smaller powers were not expressive enough, while larger powers tended to overfit the data; 9 seemed to be a good compromise:
    ```
	ggplot(productSalesFor2013, aes(x = index, y = value)) +
	ggtitle("Product Sales for Product 191 in 2013") +
	xlab("Date") +
	ylab("Units Sold") +
	scale_x_datetime(labels = date_format("%Y-%m"), breaks = date_breaks("months")) +
	theme(axis.text.x = element_text(angle = 45)) +
	geom_point(color = "black", alpha = 0.2) +
	geom_smooth(method = "lm", formula = y ~ poly(x, 9))
    ```
06. Run the code in this cell and examine the curve. Note any peaks and troughs (they might be gentle slopes and dips).
07. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	## Is there a pattern to these sales?
	Focus on sales for 2013.
    ```
08. Insert another code cell at the end of the notebook, and add the following code. This code filters the data and creates a new dataset containing only the sales for product 191 for 2013. The code then uses this data to plot the same graph as before:
    ```
	firstDay <- as.POSIXct("2013-01-01")
	lastDay <- as.POSIXct("2013-12-31")
	day1 <- as.numeric(firstDay)
	dayN <- as.numeric(lastDay)
	productSalesFor2013 <- filter(graphData, (index >= day1) & (index <= dayN))
	options(repr.plot.width=15, repr.plot.height=5)
	ggplot(productSalesFor2013, aes(x = index, y = value)) +
	ggtitle("Product Sales for Product 191 in 2013") +
	xlab("Date") +
	ylab("Units Sold") +
	scale_x_datetime(labels = date_format("%Y-%m"), breaks = date_breaks("months")) +
	theme(axis.text.x = element_text(angle = 45)) +
	geom_point(color = "blue")
    ```
09. Run the code in this cell.
10. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	The graph and curve for 2014.
    ```
11. Insert another code cell at the end of the notebook, and add the following code. This code plots the graph and curve for 2014, using the same formula as before:
    ```
	firstDay <- as.POSIXct("2014-01-01")
	lastDay <- as.POSIXct("2014-12-31")
	day1 <- as.numeric(firstDay)
	dayN <- as.numeric(lastDay)
	productSalesFor2014 <- filter(graphData, (index >= day1) & (index <= dayN))
	ggplot(productSalesFor2014, aes(x = index, y = value)) +
	ggtitle("Product Sales for Product 191 in 2014") +
	xlab("Date") +
	ylab("Units Sold") +
	scale_x_datetime(labels = date_format("%Y-%m"), breaks = date_breaks("months")) +
	theme(axis.text.x = element_text(angle = 45)) +
	geom_point(color = "black", alpha = 0.2) +
	geom_smooth(method = "lm", formula = y ~ poly(x, 9))
    ```
12. Run the code in this cell and examine the curve. Does the graph show peaks and troughs in the same places as the graph for 2013?
13. Change the new cell appended to the notebook to a Markdown cell and add the following text to the cell:
    ```
	The graph and curve for 2015.
    ```
14. Insert another code cell at the end of the notebook, and add the following code. This code repeats the previous steps, but for 2015:
    ```
	firstDay <- as.POSIXct("2015-01-01")
	lastDay <- as.POSIXct("2015-12-31")
	day1 <- as.numeric(firstDay)
	dayN <- as.numeric(lastDay)
	productSalesFor2015 <- filter(graphData, (index >= day1) & (index <= dayN))
	ggplot(productSalesFor2015, aes(x = index, y = value)) +
	ggtitle("Product Sales for Product 191 in 2015") +
	xlab("Date") +
	ylab("Units Sold") +
	scale_x_datetime(labels = date_format("%Y-%m"), breaks = date_breaks("months")) +
	theme(axis.text.x = element_text(angle = 45)) +
	geom_point(color = "black", alpha = 0.2) +
	geom_smooth(method = "lm", formula = y ~ poly(x, 9))
    ```
15. Run the code in this cell and examine the curve. Compare the curve to that of the previous two graphs.

#### Task 5: Run the Jupyter notebook
1.  On the **Kernel** menu, click **Restart & Run All**. Allow the notebook to clear all the data and run the notebook from the start. The result should be fully documented code explaining the process that you followed.
2.  In the toolbar, click **Save and Checkpoint**.
3.  On the **File** menu, click **Close and Halt**.

**Results**: At the end of this exercise, you will have created datasets containing the top 20 products that comprise the most units sold and the top 20 products that constitute the most sales value. You will also have created graphs that illustrate how the sales of these products vary across the year, and you will have documented the entire process in a Jupyter notebook

## Exercise 3: Analyzing data using Python

### Scenario
In the lab for module 6, you analyzed incomes in the census dataset by using Boosted Decision Tree Regression. You decide to repeat the exercise, but by using the AdaBoost algorithm. This algorithm is one of the ensemble methods in the Python Scikit-learn package, so you need to incorporate Python code into a machine learning experiment. You will also refine the variables used by the regression to assess their impact on the accuracy of the model generated.

The main tasks for this exercise are as follows:
1. Create the experiment and prepare the data
2. Split the data and perform Ada Boosted Regression
3. Run the experiment and examine the results

#### Task 1: Create the experiment and prepare the data
>**Note:** You can find the comments and code for this exercise in the file **Python Data.txt**, located in the **E:\\Labfiles\\Lab08** folder.

1.  Using Microsoft Azure Machine Learning Studio, create a new experiment and add the **Adult Census Income Binary Classification** dataset module to the experiment.
2.  Add a **Select Columns in Dataset** module to the experiment, and connect the output port of the **Adult Census Income Binary Classification** dataset module to the input port of this module.
3.  Using the column selector wizard for the Select Columns in Dataset module, select the **age**, **education-num**, **sex**, **native-country**, **income**, and **fnlwgt** columns.
4.  Add an **Execute** **Python Script** module to the experiment. Set the Python version for this module to **Anaconda 4.0/Python 3.5**, and then connect the output port of the **Select Columns in Dataset** module to the leftmost input port of this module.
5.  Use the **popout script editor** for the **Execute Python Script** module to replace the **azureml\_main** function with the following code:
    ```
	def azureml_main(dataframe1 = None):
	import pandas as pd
	# Only use the columns of interest and restrict the rows to those in the US
	censusData = dataframe1[['age', 'fnlwgt', 'education-num', 'sex', 'native-country', 'income']]
	censusData = censusData.loc[censusData['native-country'] == "United-States"]
	del censusData['native-country']
	# factorize the sex column
	censusData['sex'], sex_index = pd.factorize(censusData['sex'])
	return censusData
    ```
This code performs the following tasks:
    - It filters the data to only include rows where the native country is the United States.
    - It removes the native country column from the dataset because it is no longer necessary.
    - It converts the **sex** column into numeric factors. The Scikit-learn AdaBoost function requires predictor variables to be numeric.
    - It returns the modified census dataset.
6.  Run the experiment, and verify that it completes without any errors.
7.  Examine the result dataset of the **Execute Python Script** module. Verify that the result dataset only contains the **age**, **fnlwgt**, **education-num**, **sex**, and **income** variables. Note that the sex column is now numeric (0, and 1) where previously it was text (Male, Female).

#### Task 2: Split the data and perform Ada Boosted Regression
>**Note:** You can find the comments and code for this exercise in the file **Python Model.txt**, located in the **E:\\Labfiles\\Lab08** folder.

01. Add a **Split Data** module to the experiment. Set the fraction of rows in the first output dataset to 0.9. Connect the leftmost output port of the **Execute Python Script** module to the input port of this module.
02. Add another **Execute Python Script** module to the experiment. Connect the left output port of the **Split Data** module to the leftmost input port of the **Execute Python Script** module. Connect the right output port of the **Split Data** module to the middle input port of the **Execute Python Script** module.
03. Open the **popout script editor** for the new **Execute Python Script** module and delete all the code.
04. Add the following statements to the (now blank) script:
    ```
	# Create and test a model to predict income based on age, working class, education, and gender
	import pandas as pd
	import sklearn.ensemble as ensemble
	import sklearn.metrics as metrics
	import matplotlib
	matplotlib.use("agg")
	import matplotlib.pyplot as plt
    ```
This code imports the modules required by this scripts, and also enables the **matplotlib** library to render graphics that can be captured as part of the script output.
05. Add the following code to the script:
    ```
	def azureml_main(censusTraining = None, censusTest = None):
	   # Train the model
	   classifier = ensemble.AdaBoostClassifier()
	   classifier.fit(censusTraining[["age", "fnlwgt", "education-num", "sex"]], censusTraining["income"])
    ```
This code defines the **azureml\_main** function. This function takes two parameters; a training dataset and a test dataset (the two datasets generated by the **Split Data** module). The function then creates an **AdaBoostClassifier** object, and uses it to fit a model that classifies income based on the data in the **age**, **fnlwgt**, **education-num**, and **sex** variables.
>**Note:** It is important to retain the same indentation as shown in this code. Python uses indentation to determine when the code for a function is complete. If you indent the code differently, the code might not run correctly.
06. Add the following code to the script:
    ```
	# Make predictions using the model and test data
	predictions = classifier.predict(censusTest[["age", "fnlwgt", "education-num", "sex"]])
    ```
This code uses the model to make and assess predictions using the test dataset.
07. Add the following code to the script:
    ```
	# Examine the confusion matrix to assess the accuracy of the model
	conf = metrics.confusion_matrix(censusTest["income"], predictions)
	print(pd.crosstab(censusTest["income"], predictions, rownames=['True'], colnames=['Predicted'], margins=True))
    ```
This code retrieves the confusion matrix from the predicted results and displays it as a crosstab.
08. Add the following code to the script:
    ```
	# Visualize the confusion matrix
	plt.title('Confusion Matrix')
	plt.imshow(conf, cmap='binary', interpolation='None')
	plt.savefig("confusion.png")
	plt.clf()
    ```
This code generates a graphical representation of the confusion matrix using the **matplotlib** library.
09. Add the following code to the script:
    ```
	# Calculate the ROC AUC score
	censusTest['income'], income_index = pd.factorize(censusTest['income'])
	predictedIncomes = pd.factorize(predictions)
	rocData = metrics.roc_curve(censusTest["income"], predictedIncomes[0])
	rocAuc = metrics.roc_auc_score(censusTest["income"], predictedIncomes[0])
    ```
This code generates the datapoints for a ROC curve for the predicted results against the real data in the test dataset. It then calculates the AUC for this curve.
10. Add the following code to the script:
    ```
	# Plot the ROC curve
	plt.plot(rocData[0], rocData[1], label='ROC curve (area = %0.3f)' % rocAuc)
	plt.plot([0, 1], [0, 1], 'k--') # random predictions curve
	plt.xlim([0.0, 1.0])
	plt.ylim([0.0, 1.0])
	plt.xlabel('False Positive Rate')
	plt.ylabel('True Positive Rate')
	plt.title('Receiver Operating Characteristic')
	plt.legend(loc="lower right")
	plt.show()
	plt.savefig("roc.png")
	return censusTraining
    ```
This code generates a graph that depicts the ROC curve for the predictions.
11. Save the code and return to the experiment canvas.

#### Task 3: Run the experiment and examine the results
1.  Run the experiment. There should be no errors reported.
2.  Examine the output from the Python Device (right-click the **Execute Python Script** module, click **Python Device**, and then click **Visualize)**.
3.  Scroll down to the graphics section in the output. You should see a graphical representation of the confusion matrix, followed by the ROC curve for the model.
4.  Close the **Visualization** window and then save the experiment.

**Results**: At the end of this exercise, you will have used Execute Python Script modules in a Machine Learning experiment.

**Question:** What conclusions can you draw from the confusion matrix for the census income predictions about the accuracy of the model?

**Question:** What value for the AUC does the ROC curve show?

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
