# Module 4: Preparing Data for use with Azure Machine Learning
# Lab: Preparing data for use with Machine Learning

### Scenario
You work as a data scientist for Adatum Consultants, a company that provides machine learning services and advice for a range of clients. One of your clients is Wide World Importers, who are transitioning from a Business Intelligence practice to a Business Analytics practice. As part of this transition, Wide World Importers are looking for insights from operational data sources throughout the organization.
You are working with a team of business analysts from Wide World Importers who write reports using data that is stored in a variety of locations, including Azure SQL Database. You are following the recommended steps of the Team Data Science Process (TDSP). As part of the TDSP, you will explore and prepare the data, as per the Data Acquisition and Understanding phase of the process. You will be using Power BI and Machine Learning with this team so, to familiarize yourself with the environment, you need to explore the steps required in Power BI to identify data issues—and then to perform some data preprocessing steps in Machine Learning Studio.

### Objectives
After completing this lab, you will be able to:
-   Explore data using Power BI.
-   Preprocess data using Machine Learning Studio.

### Lab Setup
Estimated Time: 60 Minutes
Virtual machine: **20774A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

## Exercise 1: Explore data using Power BI

### Scenario
Before you start to clean the data for use in business scenarios, you have to explore the data so you can understand any potential issues. In this exercise, you will connect to Azure SQL Database using Power BI, and then use the Power BI data pane to explore the data, and create charts.

The main tasks for this exercise are as follows:
1. Use Power BI to connect to Azure SQL Database
2. Use Power BI to explore and chart data stored in Azure SQL Database

#### Task 1: Use Power BI to connect to Azure SQL Database
-   Start Power BI Desktop, and connect to the Azure SQL Database you created in Lab 3.

#### Task 2: Use Power BI to explore and chart data stored in Azure SQL Database
1.  In Power BI Desktop, load the following tables:
    - Application Cities
    - Application.Countries
    - Sales.Customers
    - Sales.InvoiceLines
    - Sales.Invoices
2.  Create a bar chart, with the following data:
    - Sales InvoiceLines: LineProfit
    - Sales Invoices: InvoiceDate
3.  Copy and paste the chart on to the Power BI canvas, so you have two identical charts.
4.  On the second chart, change **Value** to **Minimum**, and set the background color to something other than white.
5.  Confirm that the chart shows that the largest loss of line profit took place in 2016.

**Results**: At the end of this exercise, you will have used Power BI to connect to Azure SQL Database, and then used Power BI to explore the data.

## Exercise 2: Preprocess data using Machine Learning Studio

### Scenario
Now you have explored your data, you can start to clean the data. In this exercise, you will use several approaches for cleaning data: remove duplicate rows, and then deal with any missing data by replacing data with the median; replace data with an estimated value, and remove all rows that contain missing data. You will then normalize some data; some of the Transaction Amounts are very large, and others are close to zero, so you will look at what happens when you normalize the data between 0 and 1. Finally, you will take a different approach to the Transaction Amount variance, and make an assumption that the transaction data that contains zeros is a mistake. You will clip those values to remove all values below a certain threshold.

The main tasks for this exercise are as follows:
1. Clean a Machine Learning dataset
2. Normalize a Machine Learning dataset
3. Remove outliers from a Machine Learning dataset

#### Task 1: Clean a Machine Learning dataset
01. In the Machine Learning Studio page, open the **Import Data into Azure ML** experiment.
02. Save this experiment as **Process Data in Azure ML**.
03. Remove the left-hand **Import Data** module (the one used to import data from Azure Blob storage).
04. Remove the **Convert to Dataset** modules, and the **Split Data** module.
05. Add the **Remove Duplicate Rows** module to the experiment canvas, and connect the input port of the **Remove Duplicate Rows** module to the output port of the **Import Data** module.
06. Add all available columns to the **Remove Duplicate Rows** module.
07. Add the **Clean Missing Data** module to the experiment canvas.
08. Connect the input port of the **Clean Missing Data** module to the output port of the **Remove Duplicate Rows** module.
09. Run the experiment, and note that, when the experiment has finished running, you will get a failure from the **Clean Missing Data** module, because you did not specify what to do with missing data.
10. Visualize the output from the **Summarize Data** module.
11. Note that there are 97,547 Customer Names in total, but only 97,147 Transaction Amounts—so there are 400 rows with missing values. This is confirmed in the **Missing Values Count** column. Four columns are missing data:
    - TransactionAmount
    - OutstandingBalance
    - TaskAmount
    - TransactionDate
12. Copy and paste the **Clean Missing Data** module twice, so that there are three instances of this module on the experiment canvas.
13. Drag the **Clean Missing Data** modules, so that they’re on the same horizontal plane, and set equally apart.
14. Connect the output port of the **Remove Duplicate Rows** task to each of the input ports of each **Clean Missing Data** module.
15. For the left-hand **Clean Missing Data** module, ensure that no columns are affected by rules, and that **column type** is included and set to **Numeric**.
16. For the central **Clean Missing Data** module, set the **Cleaning Mode** to **Remove entire row**.
17. For the right-hand **Clean Missing Data** module, ensure that no columns are affected by rules, and that **column type** is included and set to **Numeric**.
18. For the right-hand **Clean Missing Data** module, also set the **Cleaning Mode** to **Replace using Probabilistic PCA**, and select **Generate missing value indicator**.
19. Run the experiment.
20. When the experiment has finished running, visualize the output of the **Import Data** module.
21. Note that the original data comprises 97,547 rows and five columns.
22. Visualize the output of the **Remove Duplicate Rows** module.
23. Note that there are now 97,321 rows so Machine Learning has found, and removed, 226 duplicate rows from the original dataset.
24. Visualize the output of the left-hand **Clean Missing Data** module.
25. Note that the dataset contains five columns and 97,321 rows; no rows have been removed, and any missing data will be replaced by the median of that column.
26. Visualize the output port of the central **Clean Missing Data** module.
27. Note that the dataset contains five columns and 96,921 rows; in this case, all the rows that had missing data (400 rows) have been removed.
28. Visualize the output of the right-hand **Clean Missing Data** module.
29. Note that the dataset contains 97,321 rows; no rows have been removed, and any missing data will be replaced by the PCA operation.
30. Save the experiment.

#### Task 2: Normalize a Machine Learning dataset
01. In the Machine Learning Studio page, open the **Import Data into Azure ML** experiment.
02. Save this experiment as **Normalize Data in Azure ML**.
03. Run the experiment.
04. Visualize the output of the **Summarize Data** module.
05. Note that TransactionAmount ranges from -169387 to 36829.
06. Add the **Normalize Data** module to the experiment canvas.
07. Connect the input port of the **Normalize Data** module to the output port of the left-hand **Convert to Dataset** module.
08. Configure the **Normalize Data** module, so that the **Included** **column** option is set to **column names**, and **TransactionAmount** is specified as column name.
09. Configure the **Normalize Data** module, so that the **Transformation method** is set to **MinMax**, in order to rescale TransactionAmount so that the values are between 0 and 1.
10. Copy and paste the **Normalize Data** module, and connect the input port of the pasted **Normalize Data** module to the output node of the right-hand **Convert to Dataset** module.
11. You should now have two Normalize Data tasks.
12. Add the **Summarize Data** module to the left side of the experiment canvas.
13. Connect the left-hand output port of the left-hand **Normalize Data** module, to the input port of the **Summarize Data** module.
14. Copy and paste the **Summarize Data** module to the right side of the canvas.
15. Connect the left-hand output port of the right-hand **Normalize Data** module, to the input port of the pasted **Summarize Data** module.
16. Run the experiment.
17. When the experiment has run successfully, visualize the output of the left-hand **Summarize Data** module.
18. Note that TransactionAmount now ranges from 0 to 1, with a mean of 0.799168.
19. Visualize the output of the right-hand **Summarize Data** module.
20. Note that TransactionAmount now ranges from 0 to 1, with a mean of 0.837266.
21. Save the experiment.

#### Task 3: Remove outliers from a Machine Learning dataset
01. In Machine Learning Studio, open the **Process Data in Azure ML** experiment.
02. Save this experiment as **Process Outliers in Azure ML**.
03. Remove the link between the **Import Data** and **Remove Duplicate Rows** modules.
04. Add the **Clip Values** module to the experiment canvas.
05. Connect the input (top) port of the **Clip Values** module to the output port of the **Import Data** module.
06. Connect the output (lower) port of the **Clip Values** module to the input (top) port of the **Remove Duplicate Rows** module.
07. Configure the **Clip Values** module, so that the **Included** **column** option is set to **column names**, and **TransactionAmount** is specified as column name.
08. Configure the **Clip Values** module, with the following values:
    - Set of thresholds: **ClipSubpeaks**
    - Lower threshold: **Constant**
    - Constant value for lower threshold: **75**
    - Lower substitute value: **Median**
09. Run the experiment.
10. When the experiment has run successfully, visualize the output of the **Clip Values** module.
11. Select the **TransactionAmount** column, and note that, in the **Statistics** pane, **TransactionAmount** now ranges from 75.21 to 36829, so all values below 75 have now been replaced based on the median of the dataset.
12. Save the experiment.

**Results**: At the end of this exercise, you will have cleaned a dataset, normalized a set of values, and used clipping to remove outliers from data.

**Question:** What are some of the potential benefits of using Power BI to visualize a source dataset before importing it into Machine Learning?

**Question:** Why is it important to fully understand your dataset before running any form of cleaning or transformation on that data?

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.