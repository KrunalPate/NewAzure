# Module 12: Using Machine Learning with HDInsight
# Lab: Using machine learning with HDInsight

### Scenario
You work as a data scientist for Adatum Consultants, a company that provides machine learning services and advice for a range of clients. One of your clients is planning the deployment of HDInsight to support large-scale big data analysis and machine learning applications.
You are working with a team of business analysts and, as part of your planning, you will deploy an HDInsight cluster and then use this cluster to evaluate MapReduce and Spark operations.

### Objectives
After completing this lab, you will be able to:
-   Provision an HDInsight cluster.
-   Use HDInsight for MapReduce and Spark jobs.

### Lab Setup
Estimated Time: 75 minutes
Virtual machine: **20774A-LON-DEV**
User name: **ADATUM\\AdatumAdmin**
Password: **Pa55w.rd**

## Exercise 1: Provision an HDInsight cluster

### Scenario
As part of your planning for big data processing, you must first deploy Hadoop using HDInsight.

The main tasks for this exercise are as follows:
1. Provision an HDInsight cluster
2. View the cluster configuration in the Azure Portal
3. Connect to the HDInsight cluster using SSH
4. Browse cluster storage

#### Task 1: Provision an HDInsight cluster
1.  Connect to the Azure Portal, and then sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
2.  Create a new HDInsight cluster with the following details:
    -   Cluster name: **&lt;*your name*&gt;&lt;*date*&gt;**
    -   Cluster type: **Hadoop**
    -   Operating system: **Linux**
    -   Version: (**Leave at default**)
    -   Cluster tier: **STANDARD**
    -   Cluster login username: **hadmin**
    -   Cluster login password: **Pa55w.rdPa55w.rd**
    -   SSH username: **sadmin**
    -   Resource group (create new): **hadooprg**
    -   Location: **Select your region**
    -   Storage account: Create new, **&lt;*your name*&gt;&lt;*date*&gt;**
    -   Default container: replace the suggested name with the name of your cluster (for example, **&lt;*your name&gt;&lt;*date*&gt;**).
    -   Cluster size: **Number of Worker nodes: 1**
    -   Worker node size: **A3 General Purpose**
    -   Head node size: **A3 General Purpose**
3.  The deployment might take 20–30 minutes to complete. Wait for the cluster to be provisioned and do not continue with this exercise until the status shows as **Running**.

#### Task 2: View the cluster configuration in the Azure Portal
-   Log in to the **HDInsight Cluster** blade as **hadmin** with a password of **Pa55w.rdPa55w.rd**, and then explore the cluster dashboard.

#### Task 3: Connect to the HDInsight cluster using SSH
1.  In the Azure Portal, locate the host name for your cluster, and then copy this name to the clipboard.
2.  Start a PuTTY session, using the copied cluster name.
3.  When you are prompted, enter **sadmin** and **Pa55w.rdPa55w.rd** as the credentials.

#### Task 4: Browse cluster storage
>**Note**: The commands in this task are case-sensitive. They can be copied from **E:\\Labfiles\\Lab12\\Ex1Task4Cmds.txt** and pasted by right-clicking the SSH console window.

1.  In the SSH console window, type the following command to view the contents of the root folder in the HDFS file system:
    ```
	hdfs dfs -ls /
    ```
2.  Type the following command to view the contents of the /example folder in the HDFS file system. This folder contains subfolders for sample apps, data, and JAR components:
    ```
	hdfs dfs -ls /example
    ```
3.  Type the following command to view the contents of the /example/data/gutenberg folder, which contains sample text files:
    ```
	hdfs dfs -ls /example/data/gutenberg
    ```
4.  Type the following command on one line to view the text in the ulysses.txt file:
    ```
	hdfs dfs -text /example/data/gutenberg/ulysses.txt
    ```
5.  Note that the file contains large volumes of unstructured text.

**Results**: At the end of this exercise, you will have provisioned an HDInsight cluster.

## Exercise 2: Use HDInsight for MapReduce and Spark jobs

### Scenario
In the second part of your planning, you will test some options for text processing on HDInsight. You will do this by using MapReduce and Spark commands to perform filtering and word counts on large text files.

The main tasks for this exercise are as follows:
1. Run the MapReduce word count job
2. Download and view output from the MapReduce job
3. Use the Python shell to submit commands to Spark
4. Use Python script to submit an application to Spark
5. Download and view output from Spark job

#### Task 1: Run the MapReduce word count job
>**Note**: The commands in this task are case-sensitive. They can be copied from **E:\\Labfiles\\Lab12\\Ex2Task1Cmds.txt** and pasted by right-clicking the SSH console window.

01. In the SSH console window, type the following command, and then press Enter:
    ```
	ls /usr/hdp/current/hadoop-mapreduce-client
    ```
02. Note the sample JAR files stored in the cluster head node.
03. Type the following command, and then press Enter:
    ```
	hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar
    ```
04. Note the list of MapReduce functions in the hadoop-mapreduce-examples.jar file.
05. Type the following command, and then press Enter:
    ```
	hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount
    ```
06. Note the help text for the **wordcount** function.
07. Type the following command, and then press Enter:
    ```
	hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /example/data/gutenberg/davinci.txt /example/results
    ```
08. Note that this command runs a MapReduce job to process a text file (**davinci.txt**) and store the results in the **/example/results** folder.
09. When the MapReduce job has completed, type the following command, and then press Enter:
    ```
	hdfs dfs -ls /example/results
    ```
10. Note that a file named **part-r-00000** has been created in the **results** folder.
11. Type the following command, and then press Enter:
    ```
	hdfs dfs -text /example/results/part-r-00000
    ```
12. Note the type of data in the output file.
13. Minimize the SSH console window.

#### Task 2: Download and view output from the MapReduce job
1.  Start Azure Storage Explorer, ensure that **Add an Azure Account** is selected, and then sign in using the credentials of the Microsoft account that is associated with your Azure Learning Pass subscription.
2.  Under your Azure Learning Pass subscription, under **Storage Accounts**, expand the storage account as created in the previous tasks **&lt;*your name*&gt;&lt;*date*&gt;**.
3.  Expand **Blob Containers**, and then click the Blob container as created in the previous tasks **&lt;*your name*&gt;&lt;*date*&gt;**.
4.  Browse to the **example/results** folder, double-click the **part-r-00000** text file to download it, and then open it in Notepad.
5.  View the word counts for the review data.
6.  Save the file as **MapReduceResults.tsv**.
7.  Open **MapReduceResults.tsv** in Microsoft Excel®.
8.  Sort **Column B** by **Largest to Smallest**, and note that the commonest words are “the,” “of,” and “and.”
9.  Close the part-r-00000 file, and all command windows.

#### Task 3: Use the Python shell to submit commands to Spark
>**Note**: The commands in this task are case-sensitive. They can be copied from **E:\\Labfiles\\Lab12\\Ex2Task3Cmds.txt** and pasted by right-clicking the SSH console window.

01. In the SSH console window, type the following command, and then press Enter:
    ```
	pyspark
    ```
02. Note that the Python Spark shell may take a few minutes to start.
03. Type the following command, and then press Enter:
    ```
	alltxt = sc.textFile("/example/data/gutenberg/davinci.txt")
    ```
04. Type the following command, and then press Enter:
    ```
	alltxt.count()
    ```
05. Note the number of lines of text reported in the text file.
06. Type the following command, and then press Enter:
    ```
	alltxt.first()
    ```
07. Note the first line shown from the text file.
08. Type the following command, and then press Enter:
    ```
	filtertxt = alltxt.filter(lambda alltxt: "Italy" in alltxt)
    ```
09. Note that this command creates a new RDD named **filtertxt** that filters the text so that only lines containing the word “Italy” are included.
10. Type the following command, and then press Enter:
    ```
	filtertxt.count()
    ```
11. Note that the number of rows in the **filtertxt** RDD is shown.
12. Type the following command, and then press Enter:
    ```
	filtertxt.collect()
    ```
13. Note that this command displays the contents of the **filtertxt** RDD.
14. Type the following command, and then press Enter:
    ```
	quit()
    ```
15. Press Enter again to return to the command line.

#### Task 4: Use Python script to submit an application to Spark
>**Note**: The commands in this task are case-sensitive. They can be copied from **E:\\Labfiles\\Lab12\\Ex2Task4Cmds.txt** and pasted by right-clicking the SSH console window.

01. On the taskbar, click File Explorer, and then browse to **E:\\Labfiles\\Lab12**.
02. Double-click **SparkScript.txt** to open the file in Notepad.
03. Select the contents of the file, and then copy them to the clipboard.
04. Switch to the SSH console window, type the following command, and then press Enter:
    ```
	nano SparkCount.py
    ```
05. Right-click the console window to paste the script that you copied previously.
06. Verify that the editor contains the following code:
    ```
	# import and initialize Spark context
	from pyspark import SparkConf, SparkContext
	cnf = SparkConf().setMaster("local").setAppName("SparkCount")
	sc = SparkContext(conf = cnf)
	# use Spark to count words
	alltxt = sc.textFile("/example/data/gutenberg/outlineofscience.txt")
	words = alltxt.flatMap(lambda alltxt: alltxt.split(" "))
	counts = words.map(lambda word: (word, 1)).reduceByKey(lambda a, b: a +b)
	# store results
	counts.saveAsTextFile("/example/spark_output")
    ```
07. Press CTRL+X, and then press Y and Enter to save SparkCount.py.
08. Type the following command, and then press Enter:
    ```
	spark-submit SparkCount.py
    ```
09. Note that the SparkCount.py Python script has been submitted to Spark.
10. Wait for the script to complete, type the following command, and then press Enter:
    ```
	hdfs dfs -ls /example/spark_output
    ```
11. Note the list of output files.
12. Type the following command, and then press Enter:
    ```
	hdfs dfs -text /example/spark_output/part-00000
    ```
13. Note that the **part-00000** file contains word counts.
14. Minimize the SSH console window.

#### Task 5: Download and view output from Spark job
1.  Switch to Azure Storage Explorer, and then browse to the **example** folder.
2.  Double-click **spark\_output**, and then open this file in Notepad.
3.  Save the file as **SparkResults.csv**, and then open it in Excel.
4.  Browse through the first few rows of the data. Note that some further processing would be needed, for example, to remove brackets and control characters.
5.  Close Microsoft Excel without saving your changes.
6.  Close the SSH console session.

**Results**: At the end of this exercise, you will have run a MapReduce job and used the Python shell to run Spark commands.

**Question:** In your own organization, why might you choose Windows over Linux as the operating system for an HDInsight cluster?

**Question:** True or false: To create a basic HDInsight cluster, you select Hadoop as the cluster type.

©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
