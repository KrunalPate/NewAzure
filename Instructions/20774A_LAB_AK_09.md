# Module 9: Initializing and Optimizing Machine Learning Models

## Lab: Initializing and optimizing Machine Learning models

### Exercise 1: Using hyperparameters

#### Task 1: Create a model with tuned parameters

01. On the **20774A-LON-DEV** virtual machine, ensure that you are logged in as **ADATUM\\AdatumAdmin**.
02. In Internet Explorer, in the address bar, type **https://studio.azureml.net/**.
03. On the **Microsoft Azure Machine Learning Studio** page, click **Sign In**.
04. If you are prompted for credentials, use the details of the Microsoft account that you created for this course.
05. In Machine Learning Studio, ensure that **EXPERIMENTS** is selected in the navigation pane, click **+ NEW**, and then click **Blank Experiment**.
06. In the **Search experiment items** box, start to type **Adult**, and then from the module list, drag the **Adult Census Income Binary Classification dataset** module onto the experiment canvas.
07. In the **Search experiment items** box, start to type **Split**, and then from the module list, drag the **Split Data** module onto the experiment canvas, below the dataset module.
08. Connect the output of the dataset module to the input node of the **Split Data** module.
09. Click the **Split Data** module, and then in the **Properties** pane, in the **Fraction of rows in the first output dataset** box, type **0.8**.
10. In the **Search experiment items** box, start to type **Vector**, and then from the module list, drag the **Two-Class Support Vector Machine** module onto the experiment canvas, to the left of the **Split Data** module.
11. Click the **Two-Class Support Vector Machine** module, and then in the **Properties** pane, configure the following properties:
  - Create trainer mode: **Parameter range**
  - Use Range Builder: **Not selected**
  - Number of iterations: **5,10,15**
Leave all other values at their defaults.
12. To retain a proportion of the training set for validation, in the **Search experiment items** box, start to type **Split**, and then from the module list, drag a second **Split Data** module onto the experiment canvas, below the existing **Split Data** module.
13. Connect the left output of the first **Split Data** module to the input node of the new **Split Data** module.
14. Click the **Split Data** module, and then in the **Properties** pane, in the **Fraction of rows in the first output dataset** box, type **0.7**.
15. In the **Search experiment items** box, start to type **Tune**, and then from the module list, drag the **Tune Model Hyperparameters** module onto the experiment canvas, below the **Two-Class Support Vector Machine** and second **Split Data** modules.
16. Click the **Tune Model Hyperparameters** module, and then in the **Properties** pane, in **Specify parameter sweeping mode**, click **Entire grid**.
17. Click **Launch column selector**.
18. In the text box, type **income**, press Enter, and then click the check mark.
19. Connect the output of the **Two-Class Support Vector Machine** module to the left input of the **Tune Model Hyperparameters** module.
20. Connect the left output of the second **Split Data** module (created in step 12) to the center input of the **Tune Model Hyperparameters** module.
21. Connect the right output of the second **Split Data** module to the right input of the **Tune Model Hyperparameters** module.
22. In the **Search experiment items** box, start to type **Score**, and then from the module list, drag the **Score Model** module onto the experiment canvas, below the **Tune Model Hyperparameters** module.
23. Connect the right output of the **Tune Model Hyperparameters** module (the trained best model) to the left input of the **Score Model** module.
24. Connect the right output of the first **Split Data** module (the test dataset) to the right input of the **Score Model** module.
25. In the **Search experiment items** box, start to type **Evaluate**, and then from the module list, drag the **Evaluate Model** module onto the experiment canvas, below the **Score Model** module.
26. Connect the output of the **Score Model** module to the left input of the **Evaluate Model** module.
27. At the bottom of the page, click **SAVE AS**.
28. In the **EXPERIMENT NAME** box, type **Hyperparameters Test**, and then click the check mark.

#### Task 2: Add an untuned model

01. In the **Search experiment items** box, start to type **Vector**, and then from the module list, drag a second **Two-Class Support Vector Machine** module onto the experiment canvas, to the right of the second **Split Data** module. Leave the settings for the second **Two-Class Support Vector Machine** module at their defaults.
02. In the **Search experiment items** box, start to type **Train**, and then from the module list, drag the **Train Model** module onto the experiment canvas, to the right of the **Tune Model Hyperparameters** module.
03. Click the **Train Model** module, and then click **Launch column selector**.
04. In the text box, type **income**, press Enter, and then click the check mark.
05. Connect the output of the second **Two-Class Support Vector Machine** module to the left input of the **Train Model** module.
06. Connect the left output of the second **Split Data** module (created in step 12 of the previous task) to the right input of the **Train Model** module.
07. In the **Search experiment items** box, start to type **Score**, and then from the module list, drag the **Score Model** module onto the experiment canvas, below the **Train Model** module.
08. Connect the output of the **Train Model** module to the left input of the new **Score Model** module.
09. Connect the right output of the first **Split Data** module (the test dataset) to the right input of the new **Score Model** module.
10. Connect the output of the **Score Model** module to the right input of the **Evaluate Model** module.

#### Task 3: Compare tuned and untuned models

1. Run the experiment by clicking **RUN** at the bottom of the page.
2. When the experiment has completed, right-click the **Evaluate Model** module, click **Evaluation results**, and then click **Visualize**.
3. Compare the metrics (the blue and red lines on the chart). Does tuning the hyperparameters improve the accuracy of the model in this case?

  The answer is that tuning hyperparameters has minimal effect in this case. The **Scored dataset** line represents the left input of the **Evaluate Model** module, and uses tuned parameters. The tuned parameters have not produced better accuracy than the untuned model (the right input, the **Scored dataset to compare** line); the ROC curves are almost the same for each input.
4. Close the visualization by clicking the **x** at the top-right of the window.

> **Results**: At the end of this exercise, you will have created an experiment that compares the effect of tuned hyperparameters with a simple classification model that does not use tuning.

Â©2017 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
